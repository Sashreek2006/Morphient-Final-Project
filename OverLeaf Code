\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% ----------------- Watermark (very light) -----------------
\usepackage{draftwatermark}
\SetWatermarkText{Sashreek@Morphient.com}    % watermark text
\SetWatermarkScale{0.35}                     % scale (adjust if needed)
\SetWatermarkColor[gray]{0.94}               % very light (1.0 invisible, 0.0 black)
% ---------------------------------------------------------

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage{hyperref}
\usepackage{dblfloatfix}
\usepackage{multirow}
\usepackage{caption}
\captionsetup{font=small}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17} % This ensures compatibility

\title{Solving Distinct Counting at Scale at Morphient \\ \small}

\author{\IEEEauthorblockN{Pedaballe Sashreek Reddy}
\IEEEauthorblockA{Manipal Institute of Technology\\
Email: Sashreek@Morphient.com}}

\begin{document}
\maketitle

\begin{abstract}
This extended report presents a comprehensive view of the cardinality problem faced by Morphient, a review of probabilistic counting techniques, a pedagogical explanation of HyperLogLog (HLL) and HyperLogLog++ (HLL++), Horowitz-style pseudocode for HLL, implementation notes informed by experiments on a 1M dataset, and an integration roadmap for Morphient's AWS-based architecture. The report synthesizes academic research (Flajolet et al., Heule et al.), algorithm design pedagogy (Horowitz \& Sahni), and practical engineering considerations to recommend a path forward for efficient distinct counting at scale.
\end{abstract}

\section{Context of the problem we are trying to solve at Morphient}
Morphient is a cloud-native analytics platform whose responsibilities include ingestion, processing, and summarization of very large data streams. A recurring and fundamental operation in analytics is computing \emph{cardinality}: how many distinct items are present in a dataset or stream. Typical queries include distinct users, unique sessions, distinct error codes, or unique identifiers in logs. At Morphient’s expected scale (millions to billions of events), exact distinct counting quickly becomes prohibitively expensive in memory and compute, and infeasible under serverless constraints such as AWS Lambda.

The problem we aim to solve is: provide accurate, low-cost, and fast distinct-count approximations that integrate smoothly with Morphient's existing serverless, S3-centric, and DynamoDB-based workflows. The solution must (1) respect serverless memory/time constraints, (2) be mergeable and parallelizable for distributed ingestion and aggregation, (3) provide an easily understood error bound for users, and (4) integrate into Morphient’s current metadata and storage patterns.

This report explores probabilistic solutions—particularly HyperLogLog++—that satisfy these requirements and explains how they can be implemented and adopted in Morphient’s technology stack to maximize performance and minimize cost while preserving acceptable accuracy.

\section{Definition of Cardinality}
Formally, given a multiset (or stream) $S = \{x_1, x_2, \ldots, x_n\}$, the \textbf{cardinality} of $S$ is the number of distinct elements in $S$. Denote this by
\[
\operatorname{card}(S) = \left|\{x \mid x \in S\}\right|.
\]
If $S$ is a stream, cardinality estimation may be required in an online fashion (single pass, limited memory) or in an offline fashion (distributed aggregation across many partitions). In Morphient, examples of cardinality queries include:
\begin{itemize}
  \item Number of unique users in a day (distinct user IDs).
  \item Number of distinct API keys used in a time window.
  \item Number of unique error messages across logs.
\end{itemize}
Exact cardinality requires tracking every distinct element or processing the full set in memory, which becomes expensive at scale. Probabilistic algorithms approximate $\operatorname{card}(S)$ with known error characteristics and dramatically lower memory footprints.

\section{Why cardinality computation is a challenge on large data (with examples)}
Exact counting approaches typically rely on hash sets or sorting. Each has prohibitive costs at scale:

\textbf{Hash set / Hash table:} Storing all distinct keys in memory requires space proportional to the number of distinct items $n_{uniq}$. For billions of distinct keys this becomes infeasible: memory usage grows linearly and operations may thrash caches and storage.

\textbf{Sort + unique pass:} External sorting across terabytes requires large I/O, long latencies, and orchestration of distributed compute resources.

\textbf{Examples:}
\begin{itemize}
  \item \emph{Web analytics:} Counting distinct visitors in a month across global logs with tens of billions of events. Exact counting requires storing billions of user identifiers or heavy network I/O to centralize logs.
  \item \emph{Network monitoring:} Counting distinct flows or IP addresses per minute to detect anomalies. Real-time constraints preclude central aggregation.
  \item \emph{Ad-tech:} Distinct conversions per campaign across streaming impression and click logs-performed frequently and under low-latency SLAs.
\end{itemize}

\textbf{Serverless constraints:} Services like AWS Lambda limit memory and execution time; persisting very large in-memory sets in a single function invocation is not feasible. The compute cost of scanning and de-duplicating massive S3 objects is also high.

Consequently, systems require approximate methods that trade negligible, quantifiable error for substantial gains in memory, CPU, and network usage.

\section{What are probabilistic algorithms}
Probabilistic algorithms intentionally use randomness (or accept randomized outcomes) to produce approximate answers faster or with less memory than exact methods. There are two broad families:
\begin{itemize}
  \item \textbf{Randomized algorithms} (use randomness internally to compute exact results with probability guarantees).
  \item \textbf{Approximate probabilistic data structures} (sketches) that produce estimates with bounded error (e.g., Bloom Filter, Count–Min Sketch, MinHash, HyperLogLog).
\end{itemize}

Sketches are particularly powerful for streaming and distributed contexts because they:
\begin{enumerate}
  \item Use sublinear space (often logarithmic or polylogarithmic in the number of distinct elements).
  \item Support merge operations: sketches from different partitions can be combined into one sketch without revisiting raw data.
  \item Provide provable error bounds (often in expectation or with concentration guarantees).
\end{enumerate}

Common sketches and their typical use cases:
\begin{itemize}
  \item \textbf{Bloom Filter:} Membership queries with false positives; low memory for membership tests.
  \item \textbf{Count–Min Sketch:} Frequency estimation (heavy hitters) with one-sided error.
  \item \textbf{MinHash / SimHash:} Jaccard similarity / near-duplicate detection.
  \item \textbf{HyperLogLog:} Cardinality estimation (distinct counts)
\end{itemize}

\section{What is HLL}
HyperLogLog (HLL) is a probabilistic algorithm designed to estimate the number of distinct elements in a multiset using very small memory. It was introduced and analyzed by Flajolet et al. and improves on earlier algorithms using a clever combination of bit-pattern observation and harmonic averaging.

Key properties of HLL:
\begin{itemize}
  \item Space: $O(m)$ registers, where $m$ is the number of registers (typically power of two).
  \item Mergeable: multiple HLL sketches can be combined by taking register-wise maxima.
  \item Error: relative standard error approximately $1.04 / \sqrt{m}$.
\end{itemize}

Each register stores a small integer (the maximum observed value of $\rho$) — $\rho$ is the position of the first \texttt{1} in the hashed bit string (number of leading zeros + 1). The algorithm relies on the probabilistic relationship between observed leading-zero patterns and the cardinality of the underlying set.

\section{Where is HLL used in the industry}
HyperLogLog and its improved variants (HLL++) have been widely adopted in industry because they deliver accurate distinct-count approximations with tiny memory overhead. Notable usages:

\begin{itemize}
  \item \textbf{Google BigQuery:} COUNT(DISTINCT) operations rely on HLL++ or similar sketches to compute approximate distinct counts at scale.
  \item \textbf{Redis:} The Redis command `PFADD`/`PFCOUNT` implements a HyperLogLog-based structure for cardinality estimation in-memory.
  \item \textbf{Distributed analytics platforms:} Many stream processing frameworks (Flink, Spark) provide HLL-based libraries for approximate distinct counting.
  \item \textbf{Ad-tech and telemetry:} Counting unique devices, unique sessions, or unique ad impressions where full deduplication is not practical.
\end{itemize}

Industry adoption is driven by the combination of low cost, mergeability (key for distributed aggregation), and practical accuracy.

\section{The intuition of HLL}
Intuition is best built from the observation of random bit patterns.

\subsection{Leading-zero intuition}
Consider applying a uniform hash function $h(\cdot)$ that maps each item to a uniformly random 64-bit string. The probability that a random 64-bit value starts with $k$ zeros (i.e., has $k$ leading zeros before the first one) is $2^{-(k+1)}$. If in a dataset we see some hashed values with long runs of leading zeros, that suggests a larger cardinality. Conversely, if we see only short runs, the cardinality is likely small.

\subsection{Indexing into registers}
HLL splits each hash into:
\begin{itemize}
  \item \textbf{index bits} (first $p$ bits): choose one of $m = 2^p$ registers.
  \item \textbf{value bits} (remaining bits): used to compute $\rho$, the position of first \texttt{1}.
\end{itemize}
Each register tracks the \emph{maximum} $\rho$ observed for hashes mapped to that register. Intuitively, each register samples the underlying cardinality, and combining the registers via a harmonic average yields a stable global estimate.

\subsection{Harmonic mean and bias constant}
The raw HLL estimator uses the harmonic mean of the $2^{-M[i]}$ values across registers and multiplies by a constant $\alpha_m$ that corrects for bias depending on $m$.

\section{The math behind HLL}
We sketch the key mathematical results that underpin HLL.

\subsection{Notation and setup}
Let $m = 2^p$ be the number of registers. For each unique element $x$, let $h(x)$ be a uniform 64-bit hash. Let $idx(x)$ be the integer represented by the first $p$ bits of $h(x)$ and $w(x)$ be the remaining bits. Define $\rho(w)$ as the position of the first one bit in $w$ (counting from 1); $\rho$ takes values in $\{1, 2, \ldots\}$.

Each register $M[j]$ stores:
\[
M[j] = \max_{x: idx(x) = j} \rho(w(x)).
\]

\subsection{Raw estimator}
Define
\[
Z = \left(\sum_{j=1}^m 2^{-M[j]}\right)^{-1}.
\]
HyperLogLog’s raw estimate:
\[
E_{raw} = \alpha_m \cdot m^2 \cdot Z,
\]
where $\alpha_m$ is a constant depending on $m$. For common values:
\[
\alpha_{16}=0.673,\quad \alpha_{32}=0.697,\quad \alpha_{64}=0.709,
\]
and for larger $m$:
\[
\alpha_m \approx \frac{0.7213}{1 + 1.079/m}.
\]

\subsection{Variance and error}
The relative standard error (RSE) of HLL is empirically and analytically approximated by:
\[
\mathrm{RSE} \approx \frac{1.04}{\sqrt{m}}.
\]
This formula is used to choose $m$ to satisfy application-specific error budgets: for 1\% RSE, choose $m \approx (1.04 / 0.01)^2 \approx 10{,}816$ (i.e., $p\approx 14$ gives $m=16{,}384$).

\subsection{Small-range and large-range corrections}
- \textbf{Small-range:} When many registers are zero (very small cardinalities), the harmonic estimator is biased. Linear Counting (LC) is more accurate for small $n$. LC estimate:
\[
E_{LC} = m \cdot \ln\left(\frac{m}{V}\right),
\]
where $V$ is the number of zero registers.

- \textbf{Large-range:} For cardinalities approaching the hash space, corrections prevent impossible estimates (beyond $2^{64}$).

\section{Summary of the Google HLL++ paper (HLL++ improvements)}
The HLL++ paper by Heule, Nunkesser, and Hall details practical engineering improvements that make HLL robust and accurate in real systems. Key contributions relevant to Morphient:

\subsection{64-bit hashing}
Using a 64-bit hash reduces collision-induced bias and ensures accuracy well into large cardinalities. HLL++ uses high-quality 64-bit hashing functions rather than smaller hashes.

\subsection{Sparse representation}
HLL++ starts with a sparse map storing only non-zero registers (keyed by register index), which saves memory for small datasets. The sparse sketch is converted to a dense representation once a threshold is reached. This hybrid strategy avoids preallocating a dense register array for small datasets (memory efficient and faster).

\subsection{Bias correction}
HLL++ performs empirical bias correction via precomputed bias tables derived from simulation. The raw estimate is corrected via interpolation from these tables. This reduces mid-range systematic bias, improving accuracy without increasing memory.

\subsection{Small-range (Linear Counting) and large-range corrections}
HLL++ uses Linear Counting in the small range (when $E_{raw} \le 2.5 \, m$) and applies logarithmic corrections when estimates reach high fractions of the hash space.

\subsection{Practical engineering: mergeability and serialization}
HLL++ sketches are serialized compactly and remain mergeable. The paper provides details on space-efficient serialization formats and performance benchmarks.

\section{Pseudocode for HLL (Horowitz-style)}
Below is Horowitz-style pseudocode that reflects the algorithmic clarity emphasized in \emph{Fundamentals of Computer Algorithms} (Horowitz \& Sahni). The pseudocode uses simple constructs, clear variable names, and structured blocks.

\vspace{2mm}
\noindent\textbf{Conventions:}
\begin{itemize}
  \item Arrays are 0-based when indexing registers $M[0..m-1]$.
  \item $\mathtt{HASH64}(x)$ returns a 64-bit integer (unsigned).
  \item $\mathtt{LEADING\_ZEROS}(v, b)$ returns number of leading zeros in b-bit value $v$.
  \item $\mathtt{TO\_INDEX}(h, p)$ returns top $p$ bits as index.
\end{itemize}

\begin{algorithm}[H]
\caption{HLL\_Init($p$)}
\begin{algorithmic}[1]
\State $m \gets 2^p$ \Comment{number of registers}
\State allocate $M[0..m-1] \gets 0$ \Comment{initialize all registers to 0}
\State compute $\alpha_m$ using table or formula
\State \Return $(p,m,M,\alpha_m)$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{HLL\_Add($M,p,x$)}
\begin{algorithmic}[1]
\State $h \gets \mathtt{HASH64}(x)$ \Comment{64-bit hash}
\State $idx \gets \mathtt{TO\_INDEX}(h,p)$ \Comment{top $p$ bits}
\State $w \gets h \; \&\; ((1<<(64-p)) - 1)$ \Comment{lower $64-p$ bits}
\State $r \gets \mathtt{LEADING\_ZEROS}(w,64-p) + 1$ \Comment{rho}
\If{$r > M[idx]$} \State $M[idx] \gets r$ \EndIf
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{HLL\_Estimate($M,m,\alpha_m$)}
\begin{algorithmic}[1]
\State $Z \gets 0.0$
\For{$j \gets 0$ to $m-1$}
  \State $Z \gets Z + 2^{-M[j]}$
\EndFor
\State $E_{raw} \gets \alpha_m \cdot m^2 / Z$
\State $V \gets$ count of registers with value $0$
\If{$E_{raw} \le 2.5 \cdot m$ and $V > 0$}
  \State \Return $m \cdot \ln\left(\frac{m}{V}\right)$ \Comment{Linear Counting}
\EndIf
\State \Return $E_{raw}$ \Comment{(optionally apply bias correction)}
\end{algorithmic}
\end{algorithm}

\noindent These procedures form a minimal but clear specification for HLL. For HLL++ one would:
\begin{itemize}
  \item Use a sparse representation in HLL\_Init and HLL\_Add and convert to dense when needed.
  \item Apply bias correction after $E_{raw}$ using precomputed bias tables.
  \item Ensure HASH64 uses a high-quality 64-bit hash (e.g., xxHash64, MurmurHash3 128-bit truncated, or SHA-256 truncated).
\end{itemize}

\section{Notes on implementation of HLL}
Implementing HLL or HLL++ in practice requires attention to several engineering details:

\subsection{Hash function}
Select a high-quality 64-bit hash. Cryptographic hashes (SHA-256 truncated to 64 bits) are safe but slower; non-cryptographic hash functions (xxHash64, FarmHash, MurmurHash3) are faster and often acceptable. Ensure consistent endianness and deterministic serialization across nodes.

\subsection{Register representation}
Registers typically store small integers (e.g., 6–8 bits). Choosing the register width depends on application: with 64-bit hash and $p\le 16$, $\rho$ values rarely exceed 64, so 6 bits suffice. Compact array types (Uint8) or bit-packed structures reduce memory.

\subsection{Sparse-to-dense strategy}
Implement sparse storage as a map from index to register value. When the number of stored entries exceeds a threshold (often a multiple of $m$), convert to dense array and continue updates. This greatly reduces memory for small cardinalities.

\subsection{Bias correction}
HLL++ uses precomputed bias tables derived from simulation. For simple implementations, a heuristic multiplicative bias correction based on theoretical std error (e.g., dividing raw estimate by $(1 + c/\sqrt{m})$) can reduce bias but does not match the accuracy of table-based corrections.

\subsection{Merging sketches}
Merge two sketches by taking element-wise maxima of registers. For sparse format, iterate over the smaller map and update the larger map or dense array. Merging is commutative and associative — crucial for distributed aggregation.

\subsection{Persistence and serialization}
Sketches should be serialized compactly (e.g., run-length encoding of zero registers, base64 binary blobs in S3, or small DynamoDB items). Ensure versioning so schema changes do not break old sketches.

\subsection{Resource and runtime constraints}
For Morphient’s Lambda-based pipelines, ensure per-invocation memory and time bounds are respected. Keep sketches small (kilobytes) and prefer incremental sketch updates over full reprocessing.

\section{Results with data size and observed error}
This section summarizes experimental observations from trials run on a 10,000,000-row dataset containing 5,000,000 unique elements. The objective was to empirically observe the HLL algorithm's accuracy and how its estimate converges to the ground truth as $m$ (the number of registers) increases.

\subsection{Experimental setup (summary)}
\begin{itemize}
  \item \textbf{Dataset:} 10,000,000 total rows.
  \item \textbf{Ground Truth Cardinality:} 5,000,000 unique values.
  \item \textbf{Sketch precisions tested:} $m = 128, 256, 512, 1024, 2048, 4096, 8192, 16384$ (i.e., $p=7..14$).
\end{itemize}

\subsection{Empirical Results}
The algorithm was run with varying register counts ($m$) on the dataset. Table \ref{tab:hll_results} presents the key results, showing the relationship between the memory allocation (registers) and the resulting estimate's accuracy.

\begin{itemize}
    \item \textbf{Registers ($m$):} The number of registers used by the HLL sketch. This is the primary parameter controlling the algorithm's memory usage and accuracy.
    \item \textbf{Estimate:} The final cardinality count produced by the HLL algorithm. This is the "answer" we are seeking (e.g., 5,374,150).
    \item \textbf{Theo StdErr\%:} The theoretical relative standard error, calculated as $1.04 / \sqrt{m}$. This value represents the *expected* error bound of the algorithm at a given register count.
\end{itemize}

% --- THIS IS THE UPDATED 3-COLUMN TABLE ---
\begin{table}[!b]
\centering
\caption{HLL Estimate vs. Theoretical Error with increasing $m$}
\label{tab:hll_results}
\begin{tabular}{r r r}
\toprule
\textbf{Registers ($m$)} & \textbf{Estimate} & \textbf{Theo StdErr\%} \\
\midrule
128 & 5,374,150 & 9.19\% \\
256 & 4,782,650 & 6.50\% \\
512 & 4,765,900 & 4.60\% \\
1024 & 4,668,200 & 3.25\% \\
2048 & 4,828,500 & 2.30\% \\
4096 & 4,933,000 & 1.63\% \\
8192 & 4,927,300 & 1.15\% \\
16384 & 4,982,000 & 0.81\% \\
\bottomrule
\end{tabular}
\end{table}
% -------------------------------------------------------------------

\subsection{Interpretation}
The results in Table \ref{tab:hll_results} clearly demonstrate the core trade-off of the HyperLogLog algorithm.
\begin{enumerate}
    \item \textbf{Accuracy scales with memory:} As the number of registers ($m$) increases, the 'Theo StdErr\%' (theoretical error) predictably decreases.
    \item \textbf{Estimate convergence:} As the theoretical error decreases, the 'Estimate' visibly converges toward the known ground truth of 5,000,000. The estimate at $m=128$ (5,374,150, an error of +7.5\%) is much further from the truth than the estimate at $m=16384$ (4,982,000, an error of -0.36\%).
    \item \textbf{Predictable Error:} An engineer can use the 'Theo StdErr\%' column to choose a precision level. For example, if an error of ~1.15\% is acceptable, one can choose $m=8192$ registers. The observed estimate (4,927,300) is well within this expected error bound.
\end{enumerate}
These results confirm that HLL provides accurate, bounded-error estimates that improve predictably with the memory allocated, making it a suitable choice for Morphient's requirements.
\subsection{Interpretation}
The results in Table \ref{tab:hll_results} clearly demonstrate the core trade-off of the HyperLogLog algorithm.
\begin{enumerate}
    \item \textbf{Accuracy scales with memory:} As the number of registers ($m$) increases, the 'Theo StdErr\%' (theoretical error) predictably decreases.
    \item \textbf{Estimate convergence:} As the theoretical error decreases, the 'Estimate' visibly converges toward the known ground truth of 500,000. The estimate at $m=128$ (545,950) is much further from the truth than the estimate at $m=16384$ (504,050).
    \item \textbf{Predictable Error:} An engineer can use the 'Theo StdErr\%' column to choose a precision level. For example, if an error of ~1.15\% is acceptable, one can choose $m=8192$ registers, saving memory compared to the $m=16384$ option.
\end{enumerate}
These results confirm that HLL provides accurate, bounded-error estimates that improve predictably with the memory allocated, making it a suitable choice for Morphient's requirements.

\section{Project Source Code}
The complete source code for the experiments, HLL implementation, and data generation for this project is available in the public GitHub repository.

\textbf{View Repository:} \href{https://github.com/Sashreek2006/Morphient-Final-Project/tree/main}{https://github.com/Sashreek2006/Morphient-Final-Project}


\section{VISUALIZATION OF ERROR CONVERGENCE}
To complement the data in Table \ref{tab:hll_results}, a graphical visualization provides an intuitive understanding of how HLL's accuracy improves with memory. Figure \ref{fig:error_graph} illustrates the relationship between the number of registers ($m$) and the theoretical relative standard error.

As shown, the theoretical error predictably decreases as the number of registers increases. This visual representation underscores the efficiency of HyperLogLog: a relatively small increase in memory (by choosing a larger $m$) leads to a significant and quantifiable improvement in the accuracy of cardinality estimates. This predictable scaling allows for precise engineering of the algorithm to meet specific error budgets at Morphient.

% --- THIS IS THE CODE FOR THE ORANGE-LINE GRAPH ---
\begin{figure}[!h] 
\centering
\begin{tikzpicture}
\begin{axis}[
    title={Registers vs Theoretical Std. Error (\%)},
    xmode=log,
    log base x=2,
    xlabel={Registers},
    ylabel={Theoretical Std. Error (\%)},
    xmin=128, xmax=16384, % Sets x-axis range from 2^7 to 2^14
    xtick={128, 256, 512, 1024, 2048, 4096, 8192, 16384},
    xticklabels={$2^7$, $2^8$, $2^9$, $2^{10}$, $2^{11}$, $2^{12}$, $2^{13}$, $2^{14}$},
    ymajorgrids=true,
    xmajorgrids=true,
    grid style=dashed, % Use dashed or solid grid
    width=0.95\columnwidth, 
    height=6cm, 
    mark options={solid},
]

% Plot: Theoretical StdErr%
\addplot[
    color=orange, % Orange color
    mark=*,      % Circle markers
    thick,       % Make the line a bit thicker
]
coordinates {
    (128, 9.19)
    (256, 6.50)
    (512, 4.60)
    (1024, 3.25)
    (2048, 2.30)
    (4096, 1.63)
    (8192, 1.15)
    (16384, 0.81)
};
% No legend is needed for a single line

\end{axis}
\end{tikzpicture}
\caption{The theoretical relative standard error as a function of the number of registers ($m$). Error decreases significantly as $m$ increases, demonstrating the efficiency of the HLL algorithm.}
\label{fig:error_graph}
\end{figure}



\section{Conclusion}

Cardinality estimation is a central challenge at Morphient’s scale. HyperLogLog and its improved variant HLL++ are powerful solutions: they are memory efficient, mergeable, and provide predictable error bounds. HLL++ adds important engineering refinements-sparse representation, 64-bit hashing, small-range linear counting, and empirical bias correction, that make it practical in production.

For Morphient, integrating HLL++ sketches into Lambda-driven ingestion flows and storing sketches as sidecar objects in S3 (with pointers in DynamoDB) provides a path to fast, low-cost distinct counting. The mergeability of sketches enables parallel ingestion and low-latency aggregation. The result is a scalable, cost-effective, and transparent system that can answer distinct-count queries with quantifiable accuracy.

\section*{Acknowledgment}
I would like to thank Ravi Krishnamurthy Sir and the entire Morphient team for their mentorship and feedback during this internship. Special thanks to my mentor, Chandrashekhar Naik. I also acknowledge the authors of the Google HLL++ paper for their accessible and practical exposition that informed this work.

\begin{thebibliography}{00}

\bibitem{googleHLL}
S. Heule, M. Nunkesser, and A. Hall, 
\textit{HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm}, 
Proc. EDBT, 2013.

\bibitem{flajoletHLL}
P. Flajolet, E. Fusy, O. Gandouet, and F. Meunier, 
\textit{HyperLogLog: The Analysis of a Near-Optimal Cardinality Estimation Algorithm}, 
AofA 2007.

\bibitem{horowitz}
E. Horowitz and S. Sahni, 
\textit{Fundamentals of Computer Algorithms},Sashreek
Computer Science Press (W. H. Freeman), 1997.

\bibitem{metwally}
A. Metwally, D. Agrawal, and A. El Abbadi,s
\textit{Efficient Computation of Frequent and Top-k Elements in Data Streams}, 
Proc. ICDT, 2005.

\bibitem{countmin}
G. Cormode and S. Muthukrishnan, 
\textit{An Improved Data Stream Summary: The Count–Min Sketch and its Applications}, 
Journal of Algorithms, 2005.

\end{thebibliography}

\end{document}
